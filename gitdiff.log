diff --git a/service/pom.xml b/service/pom.xml
index 3d4eb73..b74d21c 100644
--- a/service/pom.xml
+++ b/service/pom.xml
@@ -189,10 +189,27 @@ under the License.
             <version>2.6.1</version>
         </dependency>
 
+
+        <dependency>
+            <groupId>com.h2database</groupId>
+            <artifactId>h2</artifactId>
+        </dependency>
     </dependencies>
     <profiles>
     </profiles>
     <build>
+        <sourceDirectory>src/main/java</sourceDirectory>
+        <resources>
+            <resource>
+                <directory>src/main/resources</directory>
+            </resource>
+        </resources>
+        <testSourceDirectory>src/test/java</testSourceDirectory>
+        <testResources>
+            <testResource>
+                <directory>src/test/resources</directory>
+            </testResource>
+        </testResources>
         <plugins>
             <plugin>
                 <groupId>org.springframework.boot</groupId>
diff --git a/service/src/main/java/org/apache/griffin/core/CacheConfig.java b/service/src/main/java/org/apache/griffin/core/CacheConfig.java
new file mode 100644
index 0000000..8727dca
--- /dev/null
+++ b/service/src/main/java/org/apache/griffin/core/CacheConfig.java
@@ -0,0 +1,9 @@
+package org.apache.griffin.core;
+
+import org.springframework.cache.annotation.EnableCaching;
+import org.springframework.context.annotation.Configuration;
+
+@Configuration
+@EnableCaching
+public class CacheConfig {
+}
diff --git a/service/src/main/java/org/apache/griffin/core/GriffinWebApplication.java b/service/src/main/java/org/apache/griffin/core/GriffinWebApplication.java
index 385a686..9a20de7 100644
--- a/service/src/main/java/org/apache/griffin/core/GriffinWebApplication.java
+++ b/service/src/main/java/org/apache/griffin/core/GriffinWebApplication.java
@@ -32,7 +32,6 @@ import springfox.documentation.swagger2.annotations.EnableSwagger2;
 
 @SpringBootApplication
 @EnableScheduling
-@EnableSwagger2
 public class GriffinWebApplication/* implements CommandLineRunner*/{
     private static final Logger log = LoggerFactory.getLogger(GriffinWebApplication.class);
     public static void main(String[] args) {
diff --git a/service/src/main/java/org/apache/griffin/core/SwaggerConfig.java b/service/src/main/java/org/apache/griffin/core/SwaggerConfig.java
new file mode 100644
index 0000000..34e3e3a
--- /dev/null
+++ b/service/src/main/java/org/apache/griffin/core/SwaggerConfig.java
@@ -0,0 +1,9 @@
+package org.apache.griffin.core;
+
+import org.springframework.context.annotation.Configuration;
+import springfox.documentation.swagger2.annotations.EnableSwagger2;
+
+@Configuration
+@EnableSwagger2
+public class SwaggerConfig {
+}
diff --git a/service/src/main/java/org/apache/griffin/core/common/CacheEvictor.java b/service/src/main/java/org/apache/griffin/core/common/CacheEvictor.java
new file mode 100644
index 0000000..79d4d0a
--- /dev/null
+++ b/service/src/main/java/org/apache/griffin/core/common/CacheEvictor.java
@@ -0,0 +1,23 @@
+package org.apache.griffin.core.common;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.springframework.beans.factory.annotation.Value;
+import org.springframework.cache.annotation.CacheEvict;
+import org.springframework.scheduling.annotation.Scheduled;
+import org.springframework.stereotype.Component;
+
+@Component
+public class CacheEvictor {
+
+    private static final Logger LOGGER = LoggerFactory.getLogger(CacheEvictor.class);
+
+
+    @Scheduled(fixedRateString = "${cache.evict.hive.fixedRate}")
+    @CacheEvict(cacheNames = "hive", allEntries = true, beforeInvocation = true)
+    public void evictHiveCache() {
+        LOGGER.info("Evict hive cache");
+    }
+
+
+}
diff --git a/service/src/main/java/org/apache/griffin/core/job/JobService.java b/service/src/main/java/org/apache/griffin/core/job/JobService.java
index 911fb86..0caf2e9 100644
--- a/service/src/main/java/org/apache/griffin/core/job/JobService.java
+++ b/service/src/main/java/org/apache/griffin/core/job/JobService.java
@@ -30,13 +30,13 @@ import java.util.Map;
 
 public interface JobService {
 
-    public List<Map<String, Serializable>> getJobs();
+    List<Map<String, Serializable>> getJobs();
 
-    public GriffinOperationMessage addJob(String groupName, String jobName, String measureName, JobRequestBody jobRequestBody);
+    GriffinOperationMessage addJob(String groupName, String jobName, String measureName, JobRequestBody jobRequestBody);
 
-    public GriffinOperationMessage deleteJob(String groupName,String jobName);
+    GriffinOperationMessage deleteJob(String groupName,String jobName);
 
-    public List<JobInstance> findInstancesOfJob(String group, String name, int page, int size);
+    List<JobInstance> findInstancesOfJob(String group, String name, int page, int size);
 
-    public JobHealth getHealthInfo();
+    JobHealth getHealthInfo();
 }
diff --git a/service/src/main/java/org/apache/griffin/core/job/JobServiceImpl.java b/service/src/main/java/org/apache/griffin/core/job/JobServiceImpl.java
index 24814ef..6f6722b 100644
--- a/service/src/main/java/org/apache/griffin/core/job/JobServiceImpl.java
+++ b/service/src/main/java/org/apache/griffin/core/job/JobServiceImpl.java
@@ -112,8 +112,8 @@ public class JobServiceImpl implements JobService {
         jobInfoMap.put("measureName", jd.getJobDataMap().getString("measureName"));
         jobInfoMap.put("sourcePattern",jd.getJobDataMap().getString("sourcePattern"));
         jobInfoMap.put("targetPattern",jd.getJobDataMap().getString("targetPattern"));
-        if(StringUtils.isNotEmpty(jd.getJobDataMap().getString("dataStartTimestamp"))) {
-            jobInfoMap.put("dataStartTimestamp", jd.getJobDataMap().getString("dataStartTimestamp"));
+        if(StringUtils.isNotEmpty(jd.getJobDataMap().getString("blockStartTimestamp"))) {
+            jobInfoMap.put("blockStartTimestamp", jd.getJobDataMap().getString("blockStartTimestamp"));
         }
         jobInfoMap.put("jobStartTime",jd.getJobDataMap().getString("jobStartTime"));
         jobInfoMap.put("interval",jd.getJobDataMap().getString("interval"));
@@ -187,10 +187,10 @@ public class JobServiceImpl implements JobService {
         jobDetail.getJobDataMap().put("measureName", measureName);
         jobDetail.getJobDataMap().put("sourcePattern", jobRequestBody.getSourcePattern());
         jobDetail.getJobDataMap().put("targetPattern", jobRequestBody.getTargetPattern());
-        jobDetail.getJobDataMap().put("dataStartTimestamp", jobRequestBody.getDataStartTimestamp());
+        jobDetail.getJobDataMap().put("blockStartTimestamp", jobRequestBody.getBlockStartTimestamp());
         jobDetail.getJobDataMap().put("jobStartTime", jobRequestBody.getJobStartTime());
         jobDetail.getJobDataMap().put("interval", jobRequestBody.getInterval());
-        jobDetail.getJobDataMap().put("lastDataStartTimestamp", "");
+        jobDetail.getJobDataMap().put("lastBlockStartTimestamp", "");
     }
 
     @Override
@@ -249,6 +249,7 @@ public class JobServiceImpl implements JobService {
                 LOGGER.error("spark session "+jobInstance.getSessionId()+" has overdue, set state as unknown!\n"+e);
                 //if server cannot get session from Livy, set State as unknown.
                 jobInstance.setState(LivySessionStateMap.State.unknown);
+                jobInstanceRepo.save(jobInstance);
                 continue;
             }
             TypeReference<HashMap<String,Object>> type=new TypeReference<HashMap<String,Object>>(){};
diff --git a/service/src/main/java/org/apache/griffin/core/job/SparkSubmitJob.java b/service/src/main/java/org/apache/griffin/core/job/SparkSubmitJob.java
index ea2a860..a980c99 100644
--- a/service/src/main/java/org/apache/griffin/core/job/SparkSubmitJob.java
+++ b/service/src/main/java/org/apache/griffin/core/job/SparkSubmitJob.java
@@ -69,7 +69,7 @@ public class SparkSubmitJob implements Job {
 
     private Measure measure;
     private String sourcePattern,targetPattern;
-    private String dataStartTimestamp,lastDataStartTimestamp;
+    private String blockStartTimestamp,lastBlockStartTimestamp;
     private String interval;
     private String uri;
     private RestTemplate restTemplate = new RestTemplate();
@@ -89,17 +89,17 @@ public class SparkSubmitJob implements Job {
         String jobName=jd.getJobDataMap().getString("jobName");
         init(jd);
         //prepare current system timestamp
-        long currentDataStartTimestamp = setCurrentDataStartTimestamp(System.currentTimeMillis());
-        LOGGER.info("currentDataStartTimestamp: "+currentDataStartTimestamp);
+        long currentblockStartTimestamp = setCurrentblockStartTimestamp(System.currentTimeMillis());
+        LOGGER.info("currentblockStartTimestamp: "+currentblockStartTimestamp);
         if (StringUtils.isNotEmpty(sourcePattern)) {
             sourcePatternItems = sourcePattern.split("-");
-            setDataConnectorPartitions(measure.getSource(), sourcePatternItems, partitionItems, currentDataStartTimestamp);
+            setDataConnectorPartitions(measure.getSource(), sourcePatternItems, partitionItems, currentblockStartTimestamp);
         }
         if (StringUtils.isNotEmpty(targetPattern)) {
             targetPatternItems = targetPattern.split("-");
-            setDataConnectorPartitions(measure.getTarget(), targetPatternItems, partitionItems, currentDataStartTimestamp);
+            setDataConnectorPartitions(measure.getTarget(), targetPatternItems, partitionItems, currentblockStartTimestamp);
         }
-        jd.getJobDataMap().put("lastDataStartTimestamp", currentDataStartTimestamp + "");
+        jd.getJobDataMap().put("lastBlockStartTimestamp", currentblockStartTimestamp + "");
         setSparkJobDO();
         String result = restTemplate.postForObject(uri, sparkJobDO, String.class);
         LOGGER.info(result);
@@ -118,9 +118,9 @@ public class SparkSubmitJob implements Job {
         uri = sparkJobProps.getProperty("livy.uri");
         sourcePattern = jd.getJobDataMap().getString("sourcePattern");
         targetPattern = jd.getJobDataMap().getString("targetPattern");
-        dataStartTimestamp = jd.getJobDataMap().getString("dataStartTimestamp");
-        lastDataStartTimestamp = jd.getJobDataMap().getString("lastDataStartTimestamp");
-        LOGGER.info("lastDataStartTimestamp:"+lastDataStartTimestamp);
+        blockStartTimestamp = jd.getJobDataMap().getString("blockStartTimestamp");
+        lastBlockStartTimestamp = jd.getJobDataMap().getString("lastBlockStartTimestamp");
+        LOGGER.info("lastBlockStartTimestamp:"+lastBlockStartTimestamp);
         interval = jd.getJobDataMap().getString("interval");
     }
 
@@ -163,26 +163,26 @@ public class SparkSubmitJob implements Job {
     }
 
 
-    public long setCurrentDataStartTimestamp(long currentSystemTimestamp) {
-        long currentDataStartTimestamp=0;
-        if (StringUtils.isNotEmpty(lastDataStartTimestamp)) {
+    public long setCurrentblockStartTimestamp(long currentSystemTimestamp) {
+        long currentblockStartTimestamp=0;
+        if (StringUtils.isNotEmpty(lastBlockStartTimestamp)) {
             try {
-                currentDataStartTimestamp = Long.parseLong(lastDataStartTimestamp) + Integer.parseInt(interval) * 1000;
+                currentblockStartTimestamp = Long.parseLong(lastBlockStartTimestamp) + Integer.parseInt(interval) * 1000;
             }catch (Exception e){
-                LOGGER.info("lastDataStartTimestamp or interval format problem! "+e);
+                LOGGER.info("lastBlockStartTimestamp or interval format problem! "+e);
             }
         } else {
-            if (StringUtils.isNotEmpty(dataStartTimestamp)) {
+            if (StringUtils.isNotEmpty(blockStartTimestamp)) {
                 try{
-                    currentDataStartTimestamp = Long.parseLong(dataStartTimestamp);
+                    currentblockStartTimestamp = Long.parseLong(blockStartTimestamp);
                 }catch (Exception e){
-                    LOGGER.info("dataStartTimestamp format problem! "+e);
+                    LOGGER.info("blockStartTimestamp format problem! "+e);
                 }
             } else {
-                currentDataStartTimestamp = currentSystemTimestamp;
+                currentblockStartTimestamp = currentSystemTimestamp;
             }
         }
-        return currentDataStartTimestamp;
+        return currentblockStartTimestamp;
     }
 
     public void setSparkJobDO() {
diff --git a/service/src/main/java/org/apache/griffin/core/job/entity/JobInstance.java b/service/src/main/java/org/apache/griffin/core/job/entity/JobInstance.java
index 653cf06..2feff32 100644
--- a/service/src/main/java/org/apache/griffin/core/job/entity/JobInstance.java
+++ b/service/src/main/java/org/apache/griffin/core/job/entity/JobInstance.java
@@ -39,7 +39,7 @@ public class JobInstance extends AuditableEntity {
     State state;
     String appId;
     @Lob
-    @Column(length=1048576) //2^20=1048576
+    @Column(length=1024) //2^10=1024
     private String appUri;
     long timestamp;
 
diff --git a/service/src/main/java/org/apache/griffin/core/job/entity/JobRequestBody.java b/service/src/main/java/org/apache/griffin/core/job/entity/JobRequestBody.java
index ea04c7a..796949e 100644
--- a/service/src/main/java/org/apache/griffin/core/job/entity/JobRequestBody.java
+++ b/service/src/main/java/org/apache/griffin/core/job/entity/JobRequestBody.java
@@ -24,7 +24,7 @@ package org.apache.griffin.core.job.entity;
 public class JobRequestBody {
     private String sourcePattern;
     private String targetPattern;
-    private String dataStartTimestamp;
+    private String blockStartTimestamp;
     private String jobStartTime;
     private String interval;
 
@@ -44,12 +44,12 @@ public class JobRequestBody {
         this.targetPattern = targetPattern;
     }
 
-    public String getDataStartTimestamp() {
-        return dataStartTimestamp;
+    public String getBlockStartTimestamp() {
+        return blockStartTimestamp;
     }
 
-    public void setDataStartTimestamp(String dataStartTimestamp) {
-        this.dataStartTimestamp = dataStartTimestamp;
+    public void setBlockStartTimestamp(String blockStartTimestamp) {
+        this.blockStartTimestamp = blockStartTimestamp;
     }
 
     public String getJobStartTime() {
@@ -71,10 +71,10 @@ public class JobRequestBody {
     public JobRequestBody() {
     }
 
-    public JobRequestBody(String sourcePattern, String targetPattern, String dataStartTimestamp, String jobStartTime, String interval) {
+    public JobRequestBody(String sourcePattern, String targetPattern, String blockStartTimestamp, String jobStartTime, String interval) {
         this.sourcePattern = sourcePattern;
         this.targetPattern = targetPattern;
-        this.dataStartTimestamp = dataStartTimestamp;
+        this.blockStartTimestamp = blockStartTimestamp;
         this.jobStartTime = jobStartTime;
         this.interval = interval;
     }
@@ -90,7 +90,7 @@ public class JobRequestBody {
             return false;
         if (targetPattern != null ? !targetPattern.equals(that.targetPattern) : that.targetPattern != null)
             return false;
-        if (dataStartTimestamp != null ? !dataStartTimestamp.equals(that.dataStartTimestamp) : that.dataStartTimestamp != null)
+        if (blockStartTimestamp != null ? !blockStartTimestamp.equals(that.blockStartTimestamp) : that.blockStartTimestamp != null)
             return false;
         if (jobStartTime != null ? !jobStartTime.equals(that.jobStartTime) : that.jobStartTime != null) return false;
         return interval != null ? interval.equals(that.interval) : that.interval == null;
@@ -100,7 +100,7 @@ public class JobRequestBody {
     public int hashCode() {
         int result = sourcePattern != null ? sourcePattern.hashCode() : 0;
         result = 31 * result + (targetPattern != null ? targetPattern.hashCode() : 0);
-        result = 31 * result + (dataStartTimestamp != null ? dataStartTimestamp.hashCode() : 0);
+        result = 31 * result + (blockStartTimestamp != null ? blockStartTimestamp.hashCode() : 0);
         result = 31 * result + (jobStartTime != null ? jobStartTime.hashCode() : 0);
         result = 31 * result + (interval != null ? interval.hashCode() : 0);
         return result;
diff --git a/service/src/main/java/org/apache/griffin/core/job/repo/JobInstanceRepo.java b/service/src/main/java/org/apache/griffin/core/job/repo/JobInstanceRepo.java
index 67e893f..545eb7d 100644
--- a/service/src/main/java/org/apache/griffin/core/job/repo/JobInstanceRepo.java
+++ b/service/src/main/java/org/apache/griffin/core/job/repo/JobInstanceRepo.java
@@ -32,6 +32,14 @@ import java.util.List;
 
 @Repository
 public interface JobInstanceRepo extends CrudRepository<JobInstance,Long>{
+    /**
+     *
+     * @param group is group name
+     * @param name is job name
+     * @param pageable
+     * @return all job instances scheduled at different time using the same prototype job,
+     * the prototype job is determined by SCHED_NAME, group name and job name in table QRTZ_JOB_DETAILS.
+     */
     @Query("select s from JobInstance s " +
             "where s.groupName= ?1 and s.jobName=?2 "/*+
             "order by s.timestamp desc"*/)
diff --git a/service/src/main/java/org/apache/griffin/core/measure/MeasureService.java b/service/src/main/java/org/apache/griffin/core/measure/MeasureService.java
index aefe6aa..ac09c42 100644
--- a/service/src/main/java/org/apache/griffin/core/measure/MeasureService.java
+++ b/service/src/main/java/org/apache/griffin/core/measure/MeasureService.java
@@ -27,21 +27,21 @@ import java.util.List;
 
 public interface MeasureService {
 
-    public Iterable<Measure> getAllMeasures();
+    Iterable<Measure> getAllMeasures();
 
-    public Measure getMeasureById(long id);
+    Measure getMeasureById(long id);
 
-    public Measure getMeasureByName(String measureName);
+    Measure getMeasureByName(String measureName);
 
 
-    public GriffinOperationMessage deleteMeasureById(Long id);
+    GriffinOperationMessage deleteMeasureById(Long id);
 
 
-    public GriffinOperationMessage deleteMeasureByName(String measureName) ;
+    GriffinOperationMessage deleteMeasureByName(String measureName) ;
 
-    public GriffinOperationMessage updateMeasure(Measure measure);
+    GriffinOperationMessage updateMeasure(Measure measure);
 
-    public List<String> getAllMeasureNameByOwner(String owner);
+    List<String> getAllMeasureNameByOwner(String owner);
 
-    public GriffinOperationMessage createMeasure(Measure measure);
+    GriffinOperationMessage createMeasure(Measure measure);
 }
diff --git a/service/src/main/java/org/apache/griffin/core/measure/MeasureServiceImpl.java b/service/src/main/java/org/apache/griffin/core/measure/MeasureServiceImpl.java
index c2be174..21d90f3 100644
--- a/service/src/main/java/org/apache/griffin/core/measure/MeasureServiceImpl.java
+++ b/service/src/main/java/org/apache/griffin/core/measure/MeasureServiceImpl.java
@@ -34,7 +34,7 @@ import java.util.ArrayList;
 import java.util.List;
 
 @Service
-public class MeasureServiceImpl implements MeasureService{
+public class MeasureServiceImpl implements MeasureService {
     private static final Logger log = LoggerFactory.getLogger(MeasureServiceImpl.class);
 
     @Autowired
@@ -54,12 +54,16 @@ public class MeasureServiceImpl implements MeasureService{
     public Measure getMeasureByName(@PathVariable("measureName") String measureName) {
         return measureRepo.findByName(measureName);
     }
+
+    /*
+        TODO: require to be fixed: deleting measure doesn't deal with job protocol related to it, leading quartz to throw error that measure cannot be found.
+     */
     @Override
     public GriffinOperationMessage deleteMeasureById(@PathVariable("MeasureId") Long MeasureId) {
-        Measure temp_mesaure=measureRepo.findOne(MeasureId);
-        if (temp_mesaure==null){
+        Measure temp_mesaure = measureRepo.findOne(MeasureId);
+        if (temp_mesaure == null) {
             return GriffinOperationMessage.RESOURCE_NOT_FOUND;
-        }else {
+        } else {
             measureRepo.delete(MeasureId);
             return GriffinOperationMessage.DELETE_MEASURE_BY_ID_SUCCESS;
         }
@@ -67,11 +71,10 @@ public class MeasureServiceImpl implements MeasureService{
 
     @Override
     public GriffinOperationMessage deleteMeasureByName(@PathVariable("measureName") String measureName) {
-        Measure temp_mesaure=measureRepo.findByName(measureName);
-        if(temp_mesaure==null){
+        Measure temp_mesaure = measureRepo.findByName(measureName);
+        if (temp_mesaure == null) {
             return GriffinOperationMessage.RESOURCE_NOT_FOUND;
-        }
-        else{
+        } else {
             measureRepo.delete(temp_mesaure.getId());
             return GriffinOperationMessage.DELETE_MEASURE_BY_NAME_SUCCESS;
         }
@@ -79,25 +82,25 @@ public class MeasureServiceImpl implements MeasureService{
 
     @Override
     public GriffinOperationMessage createMeasure(@RequestBody Measure measure) {
-        String name=measure.getName();
-        Measure temp_mesaure=measureRepo.findByName(name);
-        if (temp_mesaure==null){
-            if (measureRepo.save(measure)!=null)
+        String name = measure.getName();
+        Measure temp_mesaure = measureRepo.findByName(name);
+        if (temp_mesaure == null) {
+            if (measureRepo.save(measure) != null)
                 return GriffinOperationMessage.CREATE_MEASURE_SUCCESS;
-            else{
+            else {
                 return GriffinOperationMessage.CREATE_MEASURE_FAIL;
             }
-        } else{
-            log.info("Failed to create new measure "+name+", it already exists");
+        } else {
+            log.info("Failed to create new measure " + name + ", it already exists");
             return GriffinOperationMessage.CREATE_MEASURE_FAIL_DUPLICATE;
         }
     }
 
     @Override
-    public List<String> getAllMeasureNameByOwner(String owner){
-        List<String> res=new ArrayList<String>();
-        for (Measure measure:measureRepo.findAll()){
-            if(measure.getOwner().equals(owner)){
+    public List<String> getAllMeasureNameByOwner(String owner) {
+        List<String> res = new ArrayList<String>();
+        for (Measure measure : measureRepo.findAll()) {
+            if (measure.getOwner().equals(owner)) {
                 res.add(measure.getName());
             }
         }
@@ -113,11 +116,11 @@ public class MeasureServiceImpl implements MeasureService{
 ////            System.out.print(res);
 //            return GriffinOperationMessage.UPDATE_MEASURE_SUCCESS;
 //        }
-        String name=measure.getName();
-        Measure temp_mesaure=measureRepo.findByName(name);
-        if (temp_mesaure==null){
+        String name = measure.getName();
+        Measure temp_mesaure = measureRepo.findByName(name);
+        if (temp_mesaure == null) {
             return GriffinOperationMessage.RESOURCE_NOT_FOUND;
-        }else{
+        } else {
             //in this way, id will changed
             //TODO, FRONTEND ID?
             measureRepo.delete(temp_mesaure.getId());
diff --git a/service/src/main/java/org/apache/griffin/core/measure/entity/DataConnector.java b/service/src/main/java/org/apache/griffin/core/measure/entity/DataConnector.java
index bfd6464..148e5e9 100644
--- a/service/src/main/java/org/apache/griffin/core/measure/entity/DataConnector.java
+++ b/service/src/main/java/org/apache/griffin/core/measure/entity/DataConnector.java
@@ -23,17 +23,15 @@ package org.apache.griffin.core.measure.entity;
 import com.fasterxml.jackson.annotation.JsonIgnore;
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.core.type.TypeReference;
-import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.griffin.core.util.GriffinUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import org.springframework.util.StringUtils;
 
 import javax.persistence.Entity;
 import javax.persistence.EnumType;
 import javax.persistence.Enumerated;
 import javax.persistence.Transient;
-import java.util.Collections;
-import java.util.HashMap;
+import java.io.IOException;
 import java.util.Map;
 
 @Entity
@@ -65,29 +63,20 @@ public class DataConnector extends AuditableEntity  {
     @Transient
     private Map<String,String> configInMaps;
 
-    public static Map<String,String> convertJonsToMap(String jsonString){
-        if(StringUtils.isEmpty(jsonString)) return Collections.EMPTY_MAP;
-        else{
-            Map<String, String> map = new HashMap<String, String>();
-            ObjectMapper mapper = new ObjectMapper();
-
+    public Map<String,String> getConfigInMaps() {
+        TypeReference<Map<String,String>> mapType=new TypeReference<Map<String,String>>(){};
+        if (this.configInMaps == null) {
             try {
-                map = mapper.readValue(jsonString, new TypeReference<HashMap<String, String>>() {});
-            } catch (Exception e) {
+                this.configInMaps = GriffinUtil.toEntity(config, mapType);
+            } catch (IOException e) {
                 log.error("Error in converting json to map",e);
             }
-            return map;
-
         }
-    }
-
-    public Map<String,String> getConfigInMaps() {
-        if (this.configInMaps == null) this.configInMaps = convertJonsToMap(config);
         return configInMaps;
     }
 
     public void setConfig(Map<String,String> configInMaps) throws JsonProcessingException {
-        String configJson = new ObjectMapper().writeValueAsString(configInMaps);
+        String configJson = GriffinUtil.toJson(configInMaps);
         this.config = configJson;
     }
 
@@ -111,19 +100,19 @@ public class DataConnector extends AuditableEntity  {
         this.type = type;
         this.version = version;
         this.configInMaps = config;
-        try {
-            this.config = new ObjectMapper().writeValueAsString(configInMaps);
-        } catch (JsonProcessingException e) {
-            log.error("cannot convert map to josn in DataConnector",e);
-            this.config = "";
-        }
+        this.config = GriffinUtil.toJson(configInMaps);
     }
 
     public DataConnector(ConnectorType type, String version, String config) {
         this.type = type;
         this.version = version;
         this.config = config;
-        this.configInMaps = convertJonsToMap(config);
+        TypeReference<Map<String,String>> mapType=new TypeReference<Map<String,String>>(){};
+        try {
+            this.configInMaps = GriffinUtil.toEntity(config,mapType);
+        } catch (IOException e) {
+            log.error("Error in converting json to map",e);
+        }
     }
 
     @Override
diff --git a/service/src/main/java/org/apache/griffin/core/measure/entity/Measure.java b/service/src/main/java/org/apache/griffin/core/measure/entity/Measure.java
index 2878e89..4839854 100644
--- a/service/src/main/java/org/apache/griffin/core/measure/entity/Measure.java
+++ b/service/src/main/java/org/apache/griffin/core/measure/entity/Measure.java
@@ -32,7 +32,7 @@ public class Measure extends AuditableEntity   {
 
     private String description;
 
-    public static enum MearuseType {
+    public enum MearuseType {
         accuracy,
     }
 
diff --git a/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreService.java b/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreService.java
index 26f16bc..5292a8d 100644
--- a/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreService.java
+++ b/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreService.java
@@ -26,14 +26,14 @@ import java.util.Map;
 
 public interface HiveMetastoreService {
 
-    public Iterable<String> getAllDatabases() ;
+    Iterable<String> getAllDatabases() ;
 
-    public Iterable<String> getAllTableNames(String dbName) ;
+    Iterable<String> getAllTableNames(String dbName) ;
 
-    public List<Table> getAllTable(String db) ;
+    List<Table> getAllTable(String db) ;
 
-    public Map<String,List<Table>> getAllTable() ;
+    Map<String,List<Table>> getAllTable() ;
 
-    public Table getTable(String dbName, String tableName) ;
+    Table getTable(String dbName, String tableName) ;
 
 }
diff --git a/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreServiceImpl.java b/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreServiceImpl.java
index c053438..1ea7d42 100644
--- a/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreServiceImpl.java
+++ b/service/src/main/java/org/apache/griffin/core/metastore/hive/HiveMetastoreServiceImpl.java
@@ -27,6 +27,8 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Value;
+import org.springframework.cache.annotation.CacheConfig;
+import org.springframework.cache.annotation.Cacheable;
 import org.springframework.stereotype.Service;
 import org.springframework.util.StringUtils;
 
@@ -37,6 +39,7 @@ import java.util.Map;
 
 
 @Service
+@CacheConfig(cacheNames = "hive")
 public class HiveMetastoreServiceImpl implements HiveMetastoreService{
 
     private static final Logger log = LoggerFactory.getLogger(HiveMetastoreServiceImpl.class);
@@ -53,6 +56,7 @@ public class HiveMetastoreServiceImpl implements HiveMetastoreService{
     }
 
     @Override
+    @Cacheable
     public Iterable<String> getAllDatabases() {
         Iterable<String> results = null;
         try {
@@ -66,6 +70,7 @@ public class HiveMetastoreServiceImpl implements HiveMetastoreService{
 
 
     @Override
+    @Cacheable
     public Iterable<String> getAllTableNames(String dbName) {
         Iterable<String> results = null;
         String useDbName = getUseDbName(dbName);
@@ -73,13 +78,14 @@ public class HiveMetastoreServiceImpl implements HiveMetastoreService{
             results = client.getAllTables(useDbName);
         } catch (Exception e) {
             reconnect();
-            log.error("Exception fetching tables info" + e.getMessage());
+            log.error("Exception fetching tables info: " + e.getMessage());
         }
         return results;
     }
 
 
     @Override
+    @Cacheable
     public List<Table> getAllTable(String db) {
         List<Table> results = new ArrayList<Table>();
         String useDbName = getUseDbName(db);
@@ -91,13 +97,14 @@ public class HiveMetastoreServiceImpl implements HiveMetastoreService{
             }
         } catch (Exception e) {
             reconnect();
-            log.error("Exception fetching tables info" + e.getMessage());
+            log.error("Exception fetching tables info: " + e.getMessage());
         }
         return results;
     }
 
 
     @Override
+    @Cacheable
     public Map<String,List<Table>> getAllTable() {
         Map<String,List<Table>> results = new HashMap<String, List<Table>>();
         Iterable<String> dbs = getAllDatabases();
@@ -112,7 +119,7 @@ public class HiveMetastoreServiceImpl implements HiveMetastoreService{
                 }
             } catch (Exception e) {
                 reconnect();
-                log.error("Exception fetching tables info" + e.getMessage());
+                log.error("Exception fetching tables info: " + e.getMessage());
             }
             results.put(db,alltables);
         }
@@ -121,6 +128,7 @@ public class HiveMetastoreServiceImpl implements HiveMetastoreService{
 
 
     @Override
+    @Cacheable
     public Table getTable(String dbName, String tableName) {
         Table result = null;
         String useDbName = getUseDbName(dbName);
diff --git a/service/src/main/java/org/apache/griffin/core/metastore/kafka/KafkaSchemaService.java b/service/src/main/java/org/apache/griffin/core/metastore/kafka/KafkaSchemaService.java
index 63bae35..7025a2f 100644
--- a/service/src/main/java/org/apache/griffin/core/metastore/kafka/KafkaSchemaService.java
+++ b/service/src/main/java/org/apache/griffin/core/metastore/kafka/KafkaSchemaService.java
@@ -24,16 +24,16 @@ import io.confluent.kafka.schemaregistry.client.rest.entities.Schema;
 import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaString;
 
 public interface KafkaSchemaService {
-    public SchemaString getSchemaString(Integer id);
+    SchemaString getSchemaString(Integer id);
 
-    public Iterable<String> getSubjects();
+    Iterable<String> getSubjects();
 
-    public Iterable<Integer> getSubjectVersions(String subject);
+    Iterable<Integer> getSubjectVersions(String subject);
 
-    public Schema getSubjectSchema(String subject, String version);
+    Schema getSubjectSchema(String subject, String version);
 
-    public Config getTopLevelConfig();
+    Config getTopLevelConfig();
 
-    public Config getSubjectLevelConfig(String subject);
+    Config getSubjectLevelConfig(String subject);
 
 }
diff --git a/service/src/main/java/org/apache/griffin/core/metric/MetricService.java b/service/src/main/java/org/apache/griffin/core/metric/MetricService.java
index 1b9d9c5..4d885df 100644
--- a/service/src/main/java/org/apache/griffin/core/metric/MetricService.java
+++ b/service/src/main/java/org/apache/griffin/core/metric/MetricService.java
@@ -21,5 +21,5 @@ package org.apache.griffin.core.metric;
 
 
 public interface MetricService {
-    public String getOrgByMeasureName(String measureName);
+    String getOrgByMeasureName(String measureName);
 }
diff --git a/service/src/main/resources/Init_quartz.sql b/service/src/main/resources/Init_quartz_h2.sql
similarity index 100%
rename from service/src/main/resources/Init_quartz.sql
rename to service/src/main/resources/Init_quartz_h2.sql
diff --git a/service/src/main/resources/application-dev.properties b/service/src/main/resources/application-dev.properties
index 9797634..e2a6b8a 100644
--- a/service/src/main/resources/application-dev.properties
+++ b/service/src/main/resources/application-dev.properties
@@ -33,6 +33,10 @@ spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
 spring.jpa.hibernate.naming-strategy = org.hibernate.cfg.ImprovedNamingStrategy
 
 # hive metastore
+#livy determine which spark to submit, spark knows which hive to get data, so if you change hive address here, you should check
+#livy.uri in sparkJob.properties to make the hive pointed by livy -> spark -> hive is this one you get info from.
+#Also check spark.uri in sparkJob.properties to make sure the link of application id belongs to right spark.
+#hive.metastore.uris = thrift://10.149.247.156:29083
 hive.metastore.uris = thrift://10.9.246.187:9083
 hive.metastore.dbname = default
 hive.hmshandler.retry.attempts = 15
@@ -41,4 +45,5 @@ hive.hmshandler.retry.interval = 2000ms
 # kafka schema registry
 kafka.schema.registry.url = http://10.65.159.119:8081
 
-jobInstance.fixedDelay.in.milliseconds=60000
\ No newline at end of file
+jobInstance.fixedDelay.in.milliseconds=900000
+cache.evict.hive.fixedRate=10000
\ No newline at end of file
diff --git a/service/src/main/resources/sparkJob.properties b/service/src/main/resources/sparkJob.properties
index 0247dca..2d450dd 100644
--- a/service/src/main/resources/sparkJob.properties
+++ b/service/src/main/resources/sparkJob.properties
@@ -42,8 +42,9 @@ sparkJob.jars_3=hdfs:///livy/datanucleus-rdbms-3.2.9.jar
 sparkJob.dateAndHour=dt,hour
 
 #livy
-#livy.uri=http://localhost:8998/batches
 livy.uri=http://10.9.246.187:8998/batches
+#livy.uri=http://10.149.247.156:28998/batches
 
-#spark
-spark.uri=http://10.9.246.187:8088
+#spark-admin
+#spark.uri=http://10.149.247.156:28088
+spark.uri=http://10.9.246.187:8088
\ No newline at end of file
diff --git a/service/src/test/java/org/apache/griffin/core/job/JobInstanceRepoTest.java b/service/src/test/java/org/apache/griffin/core/job/JobInstanceRepoTest.java
new file mode 100644
index 0000000..102f85b
--- /dev/null
+++ b/service/src/test/java/org/apache/griffin/core/job/JobInstanceRepoTest.java
@@ -0,0 +1,39 @@
+package org.apache.griffin.core.job;
+
+import org.apache.griffin.core.job.repo.JobInstanceRepo;
+import org.apache.griffin.core.measure.MeasureRepoTest;
+import org.apache.griffin.core.measure.repo.DataConnectorRepo;
+import org.apache.griffin.core.measure.repo.EvaluateRuleRepo;
+import org.apache.griffin.core.measure.repo.MeasureRepo;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;
+import org.springframework.boot.test.autoconfigure.orm.jpa.TestEntityManager;
+import org.springframework.context.annotation.PropertySource;
+import org.springframework.test.context.jdbc.Sql;
+import org.springframework.test.context.junit4.SpringRunner;
+
+@RunWith(SpringRunner.class)
+@PropertySource("classpath:application.properties")
+@DataJpaTest
+@Sql("classpath:test.sql")
+public class JobInstanceRepoTest {
+
+    private static final Logger LOGGER = LoggerFactory.getLogger(JobInstanceRepoTest.class);
+
+
+    @Autowired
+    private TestEntityManager testEntityManager;
+
+    @Autowired
+    private JobInstanceRepo jobInstanceRepo;
+
+    @Test
+    public void testFindByGroupNameAndJobName3Args(){
+        //jobInstanceRepo.findByGroupNameAndJobName();
+    }
+
+}
diff --git a/service/src/test/java/org/apache/griffin/core/measure/MeasureRepoTest.java b/service/src/test/java/org/apache/griffin/core/measure/MeasureRepoTest.java
new file mode 100644
index 0000000..ec2895f
--- /dev/null
+++ b/service/src/test/java/org/apache/griffin/core/measure/MeasureRepoTest.java
@@ -0,0 +1,106 @@
+package org.apache.griffin.core.measure;
+
+import org.apache.griffin.core.measure.entity.DataConnector;
+import org.apache.griffin.core.measure.entity.EvaluateRule;
+import org.apache.griffin.core.measure.entity.Measure;
+import org.apache.griffin.core.measure.repo.DataConnectorRepo;
+import org.apache.griffin.core.measure.repo.EvaluateRuleRepo;
+import org.apache.griffin.core.measure.repo.MeasureRepo;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.springframework.beans.factory.annotation.Autowired;
+import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;
+import org.springframework.boot.test.autoconfigure.orm.jpa.TestEntityManager;
+import org.springframework.context.annotation.PropertySource;
+import org.springframework.test.context.jdbc.Sql;
+import org.springframework.test.context.junit4.SpringRunner;
+import org.springframework.transaction.annotation.Propagation;
+import org.springframework.transaction.annotation.Transactional;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+@RunWith(SpringRunner.class)
+@PropertySource("classpath:application.properties")
+@DataJpaTest
+//@Sql(value = {"classpath:Init_quartz.sql", "classpath:quartz-test.sql"})
+@Sql("classpath:test.sql")
+public class MeasureRepoTest {
+
+    private static final Logger LOGGER = LoggerFactory.getLogger(MeasureRepoTest.class);
+
+
+    @Autowired
+    private TestEntityManager testEntityManager;
+
+    @Autowired
+    private MeasureRepo measureRepo;
+    @Autowired
+    private DataConnectorRepo dataConnectorRepo;
+    @Autowired
+    private EvaluateRuleRepo evaluateRuleRepo;
+
+    @Test
+    public void testFindOrganizations() {
+        Iterable<String> orgs = measureRepo.findOrganizations();
+        System.out.println(orgs);
+        for (String org : orgs) {
+            assertThat(org).isEqualTo("eBay");
+        }
+
+    }
+
+    @Test
+    public void testFindNameByOrganization() {
+        List<String> names = measureRepo.findNameByOrganization("eBay");
+        assertThat(names.get(0)).isEqualTo("buy_rates_hourly");
+        assertThat(names.get(1)).isEqualTo("griffin_aver");
+    }
+
+    @Test
+    public void testFindOrgByName() {
+        assertThat(measureRepo.findOrgByName("buy_rates_hourly")).isEqualTo("eBay");
+        assertThat(measureRepo.findOrgByName("griffin_aver")).isEqualTo("eBay");
+    }
+
+    @Test
+    @Transactional(propagation = Propagation.NOT_SUPPORTED)
+    public void testUpdateMeasure() {
+        HashMap<String, String> sourceMap = new HashMap<>();
+        sourceMap.put("database", "griffin");
+        sourceMap.put("table.name", "count");
+        DataConnector source = new DataConnector(DataConnector.ConnectorType.HIVE, "1.3", sourceMap);
+        HashMap<String, String> targetMap = new HashMap<>();
+        targetMap.put("database", "default");
+        targetMap.put("table.name", "avr_in");
+        DataConnector target = null;
+        try {
+            target = new DataConnector(DataConnector.ConnectorType.HIVE, "1.4", new ObjectMapper().writeValueAsString(targetMap));
+        } catch (IOException e) {
+            LOGGER.error("Fail to convert map to string using ObjectMapper.");
+        }
+
+        EvaluateRule rule = new EvaluateRule(0, "$source['uid'] == $target['url'] AND $source['uage'] == $target['createdts']");
+        //save before flushing
+        dataConnectorRepo.save(source);
+        dataConnectorRepo.save(target);
+        evaluateRuleRepo.save(rule);
+        measureRepo.updateMeasure((long) 1, "new desc2", "Paypal", source, target, rule);
+        for (Measure measure : measureRepo.findAll()) {
+            if (measure.getId().equals((long) 1)) {
+                assertThat(measure.getDescription()).isEqualTo("new desc2");
+                assertThat(measure.getOrganization()).isEqualTo("Paypal");
+                assertThat(measure.getSource()).isEqualTo(source);
+                assertThat(measure.getTarget()).isEqualTo(target);
+                assertThat(measure.getEvaluateRule()).isEqualTo(rule);
+            }
+        }
+
+    }
+}
diff --git a/service/src/test/resources/Init_quartz.sql b/service/src/test/resources/Init_quartz.sql
new file mode 100644
index 0000000..8537cca
--- /dev/null
+++ b/service/src/test/resources/Init_quartz.sql
@@ -0,0 +1,190 @@
+
+-- Licensed to the Apache Software Foundation (ASF) under one
+-- or more contributor license agreements.  See the NOTICE file
+-- distributed with this work for additional information
+-- regarding copyright ownership.  The ASF licenses this file
+-- to you under the Apache License, Version 2.0 (the
+-- "License"); you may not use this file except in compliance
+-- with the License.  You may obtain a copy of the License at
+-- 
+--   http://www.apache.org/licenses/LICENSE-2.0
+-- 
+-- Unless required by applicable law or agreed to in writing,
+-- software distributed under the License is distributed on an
+-- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+-- KIND, either express or implied.  See the License for the
+-- specific language governing permissions and limitations
+-- under the License.
+
+
+DROP TABLE IF EXISTS QRTZ_FIRED_TRIGGERS;
+DROP TABLE IF EXISTS QRTZ_PAUSED_TRIGGER_GRPS;
+DROP TABLE IF EXISTS QRTZ_SCHEDULER_STATE;
+DROP TABLE IF EXISTS QRTZ_LOCKS;
+DROP TABLE IF EXISTS QRTZ_SIMPLE_TRIGGERS;
+DROP TABLE IF EXISTS QRTZ_SIMPROP_TRIGGERS;
+DROP TABLE IF EXISTS QRTZ_CRON_TRIGGERS;
+DROP TABLE IF EXISTS QRTZ_BLOB_TRIGGERS;
+DROP TABLE IF EXISTS QRTZ_TRIGGERS;
+DROP TABLE IF EXISTS QRTZ_JOB_DETAILS;
+DROP TABLE IF EXISTS QRTZ_CALENDARS;
+
+CREATE TABLE QRTZ_JOB_DETAILS(
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  JOB_NAME VARCHAR(200) NOT NULL,
+  JOB_GROUP VARCHAR(200) NOT NULL,
+  DESCRIPTION VARCHAR(250) NULL,
+  JOB_CLASS_NAME VARCHAR(250) NOT NULL,
+  IS_DURABLE VARCHAR(1) NOT NULL,
+  IS_NONCONCURRENT VARCHAR(1) NOT NULL,
+  IS_UPDATE_DATA VARCHAR(1) NOT NULL,
+  REQUESTS_RECOVERY VARCHAR(1) NOT NULL,
+  JOB_DATA BLOB NULL,
+  PRIMARY KEY (SCHED_NAME,JOB_NAME,JOB_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_TRIGGERS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  TRIGGER_NAME VARCHAR(200) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  JOB_NAME VARCHAR(200) NOT NULL,
+  JOB_GROUP VARCHAR(200) NOT NULL,
+  DESCRIPTION VARCHAR(250) NULL,
+  NEXT_FIRE_TIME BIGINT(13) NULL,
+  PREV_FIRE_TIME BIGINT(13) NULL,
+  PRIORITY INTEGER NULL,
+  TRIGGER_STATE VARCHAR(16) NOT NULL,
+  TRIGGER_TYPE VARCHAR(8) NOT NULL,
+  START_TIME BIGINT(13) NOT NULL,
+  END_TIME BIGINT(13) NULL,
+  CALENDAR_NAME VARCHAR(200) NULL,
+  MISFIRE_INSTR SMALLINT(2) NULL,
+  JOB_DATA BLOB NULL,
+  PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),
+  FOREIGN KEY (SCHED_NAME,JOB_NAME,JOB_GROUP)
+  REFERENCES QRTZ_JOB_DETAILS(SCHED_NAME,JOB_NAME,JOB_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_SIMPLE_TRIGGERS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  TRIGGER_NAME VARCHAR(200) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  REPEAT_COUNT BIGINT(7) NOT NULL,
+  REPEAT_INTERVAL BIGINT(12) NOT NULL,
+  TIMES_TRIGGERED BIGINT(10) NOT NULL,
+  PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),
+  FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)
+  REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_CRON_TRIGGERS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  TRIGGER_NAME VARCHAR(200) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  CRON_EXPRESSION VARCHAR(120) NOT NULL,
+  TIME_ZONE_ID VARCHAR(80),
+  PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),
+  FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)
+  REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_SIMPROP_TRIGGERS
+(
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  TRIGGER_NAME VARCHAR(200) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  STR_PROP_1 VARCHAR(512) NULL,
+  STR_PROP_2 VARCHAR(512) NULL,
+  STR_PROP_3 VARCHAR(512) NULL,
+  INT_PROP_1 INT NULL,
+  INT_PROP_2 INT NULL,
+  LONG_PROP_1 BIGINT NULL,
+  LONG_PROP_2 BIGINT NULL,
+  DEC_PROP_1 NUMERIC(13,4) NULL,
+  DEC_PROP_2 NUMERIC(13,4) NULL,
+  BOOL_PROP_1 VARCHAR(1) NULL,
+  BOOL_PROP_2 VARCHAR(1) NULL,
+  PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),
+  FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)
+  REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_BLOB_TRIGGERS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  TRIGGER_NAME VARCHAR(200) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  BLOB_DATA BLOB NULL,
+  PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),
+  INDEX (SCHED_NAME,TRIGGER_NAME, TRIGGER_GROUP),
+  FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)
+  REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_CALENDARS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  CALENDAR_NAME VARCHAR(200) NOT NULL,
+  CALENDAR BLOB NOT NULL,
+  PRIMARY KEY (SCHED_NAME,CALENDAR_NAME))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_PAUSED_TRIGGER_GRPS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  PRIMARY KEY (SCHED_NAME,TRIGGER_GROUP))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_FIRED_TRIGGERS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  ENTRY_ID VARCHAR(95) NOT NULL,
+  TRIGGER_NAME VARCHAR(200) NOT NULL,
+  TRIGGER_GROUP VARCHAR(200) NOT NULL,
+  INSTANCE_NAME VARCHAR(200) NOT NULL,
+  FIRED_TIME BIGINT(13) NOT NULL,
+  SCHED_TIME BIGINT(13) NOT NULL,
+  PRIORITY INTEGER NOT NULL,
+  STATE VARCHAR(16) NOT NULL,
+  JOB_NAME VARCHAR(200) NULL,
+  JOB_GROUP VARCHAR(200) NULL,
+  IS_NONCONCURRENT VARCHAR(1) NULL,
+  REQUESTS_RECOVERY VARCHAR(1) NULL,
+  PRIMARY KEY (SCHED_NAME,ENTRY_ID))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_SCHEDULER_STATE (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  INSTANCE_NAME VARCHAR(200) NOT NULL,
+  LAST_CHECKIN_TIME BIGINT(13) NOT NULL,
+  CHECKIN_INTERVAL BIGINT(13) NOT NULL,
+  PRIMARY KEY (SCHED_NAME,INSTANCE_NAME))
+  ENGINE=InnoDB;
+
+CREATE TABLE QRTZ_LOCKS (
+  SCHED_NAME VARCHAR(120) NOT NULL,
+  LOCK_NAME VARCHAR(40) NOT NULL,
+  PRIMARY KEY (SCHED_NAME,LOCK_NAME))
+  ENGINE=InnoDB;
+
+CREATE INDEX IDX_QRTZ_J_REQ_RECOVERY ON QRTZ_JOB_DETAILS(SCHED_NAME,REQUESTS_RECOVERY);
+CREATE INDEX IDX_QRTZ_J_GRP ON QRTZ_JOB_DETAILS(SCHED_NAME,JOB_GROUP);
+
+CREATE INDEX IDX_QRTZ_T_J ON QRTZ_TRIGGERS(SCHED_NAME,JOB_NAME,JOB_GROUP);
+CREATE INDEX IDX_QRTZ_T_JG ON QRTZ_TRIGGERS(SCHED_NAME,JOB_GROUP);
+CREATE INDEX IDX_QRTZ_T_C ON QRTZ_TRIGGERS(SCHED_NAME,CALENDAR_NAME);
+CREATE INDEX IDX_QRTZ_T_G ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_GROUP);
+CREATE INDEX IDX_QRTZ_T_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_STATE);
+CREATE INDEX IDX_QRTZ_T_N_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP,TRIGGER_STATE);
+CREATE INDEX IDX_QRTZ_T_N_G_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_GROUP,TRIGGER_STATE);
+CREATE INDEX IDX_QRTZ_T_NEXT_FIRE_TIME ON QRTZ_TRIGGERS(SCHED_NAME,NEXT_FIRE_TIME);
+CREATE INDEX IDX_QRTZ_T_NFT_ST ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_STATE,NEXT_FIRE_TIME);
+CREATE INDEX IDX_QRTZ_T_NFT_MISFIRE ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME);
+CREATE INDEX IDX_QRTZ_T_NFT_ST_MISFIRE ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME,TRIGGER_STATE);
+CREATE INDEX IDX_QRTZ_T_NFT_ST_MISFIRE_GRP ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME,TRIGGER_GROUP,TRIGGER_STATE);
+
+CREATE INDEX IDX_QRTZ_FT_TRIG_INST_NAME ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,INSTANCE_NAME);
+CREATE INDEX IDX_QRTZ_FT_INST_JOB_REQ_RCVRY ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,INSTANCE_NAME,REQUESTS_RECOVERY);
+CREATE INDEX IDX_QRTZ_FT_J_G ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,JOB_NAME,JOB_GROUP);
+CREATE INDEX IDX_QRTZ_FT_JG ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,JOB_GROUP);
+CREATE INDEX IDX_QRTZ_FT_T_G ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP);
+CREATE INDEX IDX_QRTZ_FT_TG ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,TRIGGER_GROUP);
+
+commit;
\ No newline at end of file
diff --git a/service/src/test/resources/application.properties b/service/src/test/resources/application.properties
index d6c7851..d91302b 100644
--- a/service/src/test/resources/application.properties
+++ b/service/src/test/resources/application.properties
@@ -18,15 +18,23 @@
 #
 
 # spring.datasource.x
+
 spring.datasource.driver-class-name=org.h2.Driver
 spring.datasource.url=jdbc:h2:mem:db;DB_CLOSE_DELAY=-1
 spring.datasource.username=sa
 spring.datasource.password=sa
+#spring.datasource.url= jdbc:mysql://localhost:3306/quartz?autoReconnect=true&useSSL=false
+#spring.datasource.username =griffin
+#spring.datasource.password =123456
 
 # hibernate.X
 hibernate.dialect=org.hibernate.dialect.H2Dialect
+#spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.H2Dialect
+
 hibernate.show_sql=true
-hibernate.hbm2ddl.auto=create-drop
+spring.jpa.hibernate.ddl-auto = create-drop
+
+#hibernate.hbm2ddl.auto=create-drop
 hibernate.cache.use_second_level_cache=true
 hibernate.cache.use_query_cache=true
 hibernate.cache.region.factory_class=org.hibernate.cache.ehcache.EhCacheRegionFactory
@@ -34,6 +42,8 @@ hibernate.cache.region.factory_class=org.hibernate.cache.ehcache.EhCacheRegionFa
 # hive metastore
 hive.metastore.uris = thrift://10.9.246.187:9083
 hive.metastore.dbname = default
+hive.hmshandler.retry.attempts = 15
+hive.hmshandler.retry.interval = 2000ms
 
 # kafka schema registry
 kafka.schema.registry.url = http://10.65.159.119:8081
@@ -43,4 +53,7 @@ logging.level.root=ERROR
 logging.level.org.hibernate=ERROR
 logging.level.org.springframework.test=ERROR
 logging.level.org.apache.griffin=ERROR
-logging.file=target/test.log
\ No newline at end of file
+logging.file=target/test.log
+
+
+jobInstance.fixedDelay.in.milliseconds=60000
\ No newline at end of file
diff --git a/service/src/test/resources/quartz-test.sql b/service/src/test/resources/quartz-test.sql
new file mode 100644
index 0000000..3f5287a
--- /dev/null
+++ b/service/src/test/resources/quartz-test.sql
@@ -0,0 +1,26 @@
+INSERT INTO `QRTZ_FIRED_TRIGGERS` VALUES ('schedulerFactoryBean','LM-SHC-0095021215027875413551502787541380','measure-test-BA-test-1502789449000','BA','LM-SHC-009502121502787541355',1502789505307,1502789520000,5,'ACQUIRED',NULL,NULL,'0','0');
+
+INSERT INTO `QRTZ_JOB_DETAILS` VALUES ('schedulerFactoryBean','measure-test-BA-test-1502789449000','BA',NULL,'org.apache.griffin.core.job.SparkSubmitJob','1','1','1','0','#\n#Tue Aug 15 17:31:20 CST 2017\ninterval=40\njobStartTime=1502640000000\nsourcePattern=YYYYMMdd-HH\nmeasureName=measure-test\njobName=measure-test-BA-test-1502789449000\nblockStartTimestamp=\nlastBlockStartTimestamp=1502789480023\ntargetPattern=YYYYMMdd-HH\ngroupName=BA\n');
+
+
+INSERT INTO `QRTZ_LOCKS` VALUES ('schedulerFactoryBean','STATE_ACCESS'),('schedulerFactoryBean','TRIGGER_ACCESS');
+
+
+INSERT INTO `QRTZ_SCHEDULER_STATE` VALUES ('schedulerFactoryBean','LM-SHC-009502121502787541355',1502789504423,20000);
+
+INSERT INTO `QRTZ_SIMPLE_TRIGGERS` VALUES ('schedulerFactoryBean','measure-test-BA-test-1502789449000','BA',-1,40000,1);
+
+
+INSERT INTO `QRTZ_TRIGGERS` VALUES ('schedulerFactoryBean','measure-test-BA-test-1502789449000','BA','measure-test-BA-test-1502789449000','BA',NULL,1502789520000,1502789480000,5,'ACQUIRED','SIMPLE',1502789480000,0,NULL,0,'');
+
+
+INSERT INTO `data_connector` VALUES (9,'2017-08-15 17:30:32',NULL,'{\"database\":\"default\",\"table.name\":\"demo_src\"}','HIVE','1.2'),(10,'2017-08-15 17:30:32',NULL,'{\"database\":\"default\",\"table.name\":\"demo_tgt\"}','HIVE','1.2');
+
+
+INSERT INTO `evaluate_rule` VALUES (5,'2017-08-15 17:30:32',NULL,'$source[''id''] == $target[''id''] AND $source[''age''] == $target[''age''] AND $source[''desc''] == $target[''desc'']',0);
+
+
+INSERT INTO `job_instance` VALUES (103,'2017-08-15 17:31:21',NULL,'application_1500359928600_0025','BA','measure-test-BA-test-1502789449000',24,'success',1502789480660,'http://10.149.247.156:28088/cluster/app/application_1500359928600_0025');
+
+
+INSERT INTO `measure` VALUES (5,'2017-08-15 17:30:32',NULL,'Description of measure-test','measure-test','eBay','test','accuracy',5,9,10);
diff --git a/service/src/test/resources/test.sql b/service/src/test/resources/test.sql
new file mode 100644
index 0000000..49bd47a
--- /dev/null
+++ b/service/src/test/resources/test.sql
@@ -0,0 +1,24 @@
+-- ----------------------------
+-- Records of data_connector
+-- ----------------------------
+INSERT INTO `data_connector` VALUES ('1', '2017-07-12 11:06:47', null, '{\"database\":\"default\",\"table.name\":\"data_avr\"}', 'HIVE', '1.2');
+INSERT INTO `data_connector` VALUES ('2', '2017-07-12 11:06:47', null, '{\"database\":\"default\",\"table.name\":\"cout\"}', 'HIVE', '1.2');
+INSERT INTO `data_connector` VALUES ('3', '2017-07-12 17:40:30', null, '{\"database\":\"griffin\",\"table.name\":\"avr_in\"}', 'HIVE', '1.2');
+INSERT INTO `data_connector` VALUES ('4', '2017-07-12 17:40:30', null, '{\"database\":\"griffin\",\"table.name\":\"avr_out\"}', 'HIVE', '1.2');
+
+
+-- ----------------------------
+-- Records of evaluate_rule
+-- ----------------------------
+--INSERT INTO `evaluate_rule` VALUES ('1', '2017-07-12 11:06:47', null, '$source[\'uid\'] == $target[\'url\'] AND $source[\'uage\'] == $target[\'createdts\']', '0');
+INSERT INTO `evaluate_rule` VALUES ('1', '2017-07-12 11:06:47', null, '$source[''uid''] == $target[''url''] AND $source[''uage''] == $target[''createdts'']', '0');
+
+--INSERT INTO `evaluate_rule` VALUES ('2', '2017-07-12 17:40:30', null, '$source[\'id\'] == $target[\'id\'] AND $source[\'age\'] == $target[\'age\'] AND $source[\'desc\'] == $target[\'desc\']', '0');
+INSERT INTO `evaluate_rule` VALUES ('2', '2017-07-12 17:40:30', null, '$source[''id''] == $target[''id''] AND $source[''age''] == $target[''age''] AND $source[''desc''] == $target[''desc'']', '0');
+
+
+-- ----------------------------
+-- Records of measure
+-- ----------------------------
+INSERT INTO `measure` VALUES ('1', '2017-07-12 11:06:47', null, 'desc1', 'buy_rates_hourly', 'eBay', 'test', 'accuracy', '1', '1', '2');
+INSERT INTO `measure` VALUES ('2', '2017-07-12 17:40:30', null, 'desc2', 'griffin_aver', 'eBay', 'test', 'accuracy', '2', '3', '4');
